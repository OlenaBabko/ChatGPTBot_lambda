import datetime
import logging
import openai

# ChatGPT  Ð”Ð¾Ð¿Ð¾Ð¼Ñ–Ð¶Ð½Ð¸Ð¹ ÐºÐ»Ð°Ñ Ð´Ð»Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¸ Ð· ChatGPT
class OpenAIHelper:
# Ð†Ð½Ñ–Ñ†Ñ–Ð°Ð»Ñ–Ð·ÑƒÑ”Ð¼Ð¾ ÐºÐ»Ð°Ñ Ð· Ð¿ÐµÑ€ÐµÐ´Ð°Ð½Ð¸Ð¼Ð¸ ÐºÐ¾Ð½Ñ„Ñ–Ð³ÑƒÑ€Ð°Ñ†Ñ–ÑÐ¼Ð¸
    def __init__(self, config: dict):
        openai.api_key = config['api_key']
        self.config = config                        #:param config: Ð¡Ð»Ð¾Ð²Ð½Ð¸Ðº Ð· ÐºÐ¾Ð½Ñ„Ñ–Ð³ÑƒÑ€Ð°Ñ†Ñ–ÑÐ¼Ð¸ Ð´Ð»Ñ GPT
        self.conversations: dict[int: list] = {}    # {chat_id: history}
        self.last_updated: dict[int: datetime] = {} # {chat_id: last_update_timestamp}

# ANSWER FROM ChatGPT (:param chat_id: ÐÐ¹Ð´Ñ– Ñ‡Ð°Ñ‚Ñƒ // :param query: Ð—Ð°Ð¿Ð¸Ñ‚, ÑÐºÐ¸Ð¹ Ð¼Ð¸ Ð²Ñ–Ð´ÑÐ¸Ð»Ð°Ñ”Ð¼Ð¾ Ð´Ð¾ GPT // :return: Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ Ð²Ñ–Ð´ Ñ‡Ð°Ñ‚Ñƒ
    def get_chat_response(self, chat_id: int, query: str) -> str:
        try:
            if chat_id not in self.conversations or self.__max_age_reached(chat_id):
                self.reset_chat_history(chat_id)
            self.last_updated[chat_id] = datetime.datetime.now()

            # CHECK HISTORY SIZE TO PREVENT OVERUSE ofAPI
            if len(self.conversations[chat_id]) > self.config['max_history_size']:
                logging.info(f'Chat history for chat ID {chat_id} is too long. Summarising...')
                try:
                    summary = self.__summarise(self.conversations[chat_id])
                    logging.debug(f'Summary: {summary}')
                    self.reset_chat_history(chat_id)
                    self.__add_to_history(chat_id, role="assistant", content=summary)
                except Exception as e:
                    logging.warning(f'Error while summarising chat history: {str(e)}. Popping elements instead...')
                    self.conversations[chat_id] = self.conversations[chat_id][-self.config['max_history_size']:]

            self.__add_to_history(chat_id, role="user", content=query)

            response = openai.ChatCompletion.create(
                model=self.config['model'],
                messages=self.conversations[chat_id],
                temperature=self.config['temperature'],
                n=self.config['n_choices'],
                max_tokens=self.config['max_tokens'],
                presence_penalty=self.config['presence_penalty'],
                frequency_penalty=self.config['frequency_penalty'],
            )

            if len(response.choices) > 0:                   # ÑÐºÑ‰Ð¾ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ Ð¿Ñ€Ð¸Ð¹ÑˆÐ»Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°Ð¼Ð¸, Ñ‚ÑƒÑ‚ Ð²Ð¾Ð½Ð¸ Ð·Ð³Ñ€ÑƒÐ¿ÑƒÑŽÑ‚ÑŒÑÑ Ð² 1
                answer = ''

                if len(response.choices) > 1 and self.config['n_choices'] > 1:
                    for index, choice in enumerate(response.choices):
                        if index == 0:
                            self.__add_to_history(chat_id, role="assistant", content=choice['message']['content'])
                        answer += f'{index+1}\u20e3\n'
                        answer += choice['message']['content']
                        answer += '\n\n'
                else:
                    answer = response.choices[0]['message']['content']
                    self.__add_to_history(chat_id, role="assistant", content=answer)

                if self.config['show_usage']:                       # Ð’ main -- True Ð¿Ð¾ÐºÐ°Ð¶Ðµ ÑÐºÑ–Ð»ÑŒÐºÐ¸ Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ñ–Ð² chatGPT
                    answer += "\n\n---\n" \
                              f"ðŸ’° Tokens used: {str(response.usage['total_tokens'])}" \
                              f" ({str(response.usage['prompt_tokens'])} prompt," \
                              f" {str(response.usage['completion_tokens'])} completion)"

                return answer
            else:
                logging.error('No response from GPT-3')
                return "âš ï¸ _An error has occurred_ âš ï¸\nPlease try again in a while."

        except openai.error.RateLimitError as e:
            logging.exception(e)
            return f"âš ï¸ _OpenAI Rate Limit exceeded_ âš ï¸\n{str(e)}"

        except openai.error.InvalidRequestError as e:
            logging.exception(e)
            return f"âš ï¸ _OpenAI Invalid request_ âš ï¸\n{str(e)}"

        except Exception as e:
            logging.exception(e)
            return f"âš ï¸ _An error has occurred_ âš ï¸\n{str(e)}"

# RESET CONVERSATION
    def reset_chat_history(self, chat_id):
        self.conversations[chat_id] = [{"role": "system", "content": self.config['assistant_prompt']}]

# AGE Checks if the maximum conversation age has been reached.
    def __max_age_reached(self, chat_id) -> bool:
        if chat_id not in self.last_updated:
            return False                    # :return: A boolean indicating whether the maximum conversation age has been reached
        last_updated = self.last_updated[chat_id]
        now = datetime.datetime.now()
        max_age_minutes = self.config['max_conversation_age_minutes']
        return last_updated < now - datetime.timedelta(minutes=max_age_minutes)

# ADD TO HISTORRY
    def __add_to_history(self, chat_id, role, content):
        self.conversations[chat_id].append({"role": role, "content": content})

# SUMMARISE "Summarise this conversation in 700 characters or less"
    def __summarise(self, conversation) -> str:
        messages = [
            { "role": "assistant", "content": "Summarize this conversation in 700 characters or less" },
            { "role": "user", "content": str(conversation) }
        ]
        response = openai.ChatCompletion.create(
            model=self.config['model'],
            messages=messages,
            temperature=0.4
        )
        return response.choices[0]['message']['content']
